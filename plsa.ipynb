{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA\n",
    "probabilistic latent semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from plsa import Corpus, Pipeline, Visualize\n",
    "from plsa.pipeline import DEFAULT_PIPELINE\n",
    "from plsa.algorithms import PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'datasets/20news.csv'\n",
    "directory = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(csv_file)[\"content\"].to_csv(\"datasets/news_20_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'datasets/news_20_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline:\n",
       "========\n",
       "0: remove_non_ascii\n",
       "1: to_lower\n",
       "2: remove_numbers\n",
       "3: tag_remover\n",
       "4: punctuation_remover\n",
       "5: tokenize\n",
       "6: LemmatizeWords\n",
       "7: RemoveStopwords\n",
       "8: short_word_remover"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(*DEFAULT_PIPELINE)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus:\n",
       "======\n",
       "Number of documents: 971\n",
       "Number of words:     9865"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus.from_csv(csv_file, pipeline)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18460655, 0.15286448, 0.15064762, 0.14235353, 0.13374865,\n",
       "       0.12660758, 0.1091716 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 7\n",
    "plsa = PLSA(corpus, n_topics, True)\n",
    "#\n",
    "result = plsa.fit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Find the best PLSA model of many</b> <br>\n",
    "As with any iterative algorithm, also the probabilities in PSLA need to be (randomly) initialized prior to the first iteration step. Therefore, calling the fit method of two different PLSA instances operating on the same corpus with the same number of topics potentially leads to (slightly) different results, corresponding to different local minima of the Kullback-Leibler divergence between the true document-word probability and its approximate factorization. To mitigate this effect, perform multiple runs and pick the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18472799, 0.15843785, 0.1475734 , 0.13900792, 0.13825103,\n",
       "       0.11663721, 0.1153646 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = plsa.best_of(5) # Do a 5 different runs and pick a best of 5\n",
    "result.topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'__getstate__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\PG\\3rd sem\\Thesis\\sem_4_coding\\plsa.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PG/3rd%20sem/Thesis/sem_4_coding/plsa.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PG/3rd%20sem/Thesis/sem_4_coding/plsa.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrained_models/plsa_trained_news20_7.pkl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PG/3rd%20sem/Thesis/sem_4_coding/plsa.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(result,f)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\plsa\\pipeline.py:63\u001b[0m, in \u001b[0;36mPipeline.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PreprocessorT:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__preprocessors[name][\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: '__getstate__'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"trained_models/plsa_trained_news20_7.pkl\",\"wb\") as f:\n",
    "    pickle.dump(result,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative topic importance in new document: [0.32763389 0.070425   0.1115168  0.35108152 0.09038121 0.00567461\n",
      " 0.04328697]\n",
      "Number of previously unseen words in new document: 3\n",
      "Previously unseen words in new document: ('humpty', 'dumpty', 'funding')\n"
     ]
    }
   ],
   "source": [
    "new_doc = 'Hello! This is the federal humpty dumpty agency for state funding.'\n",
    "\n",
    "topic_components, number_of_new_words, new_words = result.predict(new_doc)\n",
    "\n",
    "print('Relative topic importance in new document:', topic_components)\n",
    "print('Number of previously unseen words in new document:', number_of_new_words)\n",
    "print('Previously unseen words in new document:', new_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting topic distribution for News20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_20=pd.read_csv(\"datasets/news_20_cleaned.csv\")[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167606</td>\n",
       "      <td>0.125942</td>\n",
       "      <td>0.293242</td>\n",
       "      <td>0.245064</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>0.041921</td>\n",
       "      <td>0.053560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.277264</td>\n",
       "      <td>0.055091</td>\n",
       "      <td>0.089403</td>\n",
       "      <td>0.127531</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.314278</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.534913</td>\n",
       "      <td>0.073154</td>\n",
       "      <td>0.097343</td>\n",
       "      <td>0.133144</td>\n",
       "      <td>0.066949</td>\n",
       "      <td>0.051206</td>\n",
       "      <td>0.043290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.494809</td>\n",
       "      <td>0.043122</td>\n",
       "      <td>0.114357</td>\n",
       "      <td>0.204409</td>\n",
       "      <td>0.034848</td>\n",
       "      <td>0.053847</td>\n",
       "      <td>0.054608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.136714</td>\n",
       "      <td>0.550428</td>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.073511</td>\n",
       "      <td>0.106030</td>\n",
       "      <td>0.055934</td>\n",
       "      <td>0.019018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>18841.0</td>\n",
       "      <td>0.260232</td>\n",
       "      <td>0.147339</td>\n",
       "      <td>0.055843</td>\n",
       "      <td>0.222390</td>\n",
       "      <td>0.171843</td>\n",
       "      <td>0.094712</td>\n",
       "      <td>0.047640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>18842.0</td>\n",
       "      <td>0.421209</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.113550</td>\n",
       "      <td>0.210476</td>\n",
       "      <td>0.119379</td>\n",
       "      <td>0.022824</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>18843.0</td>\n",
       "      <td>0.213742</td>\n",
       "      <td>0.294760</td>\n",
       "      <td>0.117598</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>0.126057</td>\n",
       "      <td>0.064816</td>\n",
       "      <td>0.043793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>18844.0</td>\n",
       "      <td>0.422876</td>\n",
       "      <td>0.059320</td>\n",
       "      <td>0.097315</td>\n",
       "      <td>0.309534</td>\n",
       "      <td>0.027330</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.045618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>18845.0</td>\n",
       "      <td>0.053519</td>\n",
       "      <td>0.493347</td>\n",
       "      <td>0.142032</td>\n",
       "      <td>0.103702</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.068138</td>\n",
       "      <td>0.014264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18146 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index   topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  \\\n",
       "0          0.0  0.167606  0.125942  0.293242  0.245064  0.072665  0.041921   \n",
       "1          1.0  0.277264  0.055091  0.089403  0.127531  0.113333  0.314278   \n",
       "2          2.0  0.534913  0.073154  0.097343  0.133144  0.066949  0.051206   \n",
       "3          3.0  0.494809  0.043122  0.114357  0.204409  0.034848  0.053847   \n",
       "4          4.0  0.136714  0.550428  0.058364  0.073511  0.106030  0.055934   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "18841  18841.0  0.260232  0.147339  0.055843  0.222390  0.171843  0.094712   \n",
       "18842  18842.0  0.421209  0.056763  0.113550  0.210476  0.119379  0.022824   \n",
       "18843  18843.0  0.213742  0.294760  0.117598  0.139233  0.126057  0.064816   \n",
       "18844  18844.0  0.422876  0.059320  0.097315  0.309534  0.027330  0.038007   \n",
       "18845  18845.0  0.053519  0.493347  0.142032  0.103702  0.124998  0.068138   \n",
       "\n",
       "        topic_6  \n",
       "0      0.053560  \n",
       "1      0.023100  \n",
       "2      0.043290  \n",
       "3      0.054608  \n",
       "4      0.019018  \n",
       "...         ...  \n",
       "18841  0.047640  \n",
       "18842  0.055800  \n",
       "18843  0.043793  \n",
       "18844  0.045618  \n",
       "18845  0.014264  \n",
       "\n",
       "[18146 rows x 8 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names=[\"index\"]+[f\"topic_{i}\" for i in range(n_topics)]\n",
    "plsa_distribution=pd.DataFrame(columns=col_names)\n",
    "missing_idx=[]\n",
    "for news,idx,append_pointer in zip(news_20,news_20.index.to_list(),range(len(news_20))):\n",
    "    try:\n",
    "        op=[idx]+list(result.predict(news)[0])\n",
    "        plsa_distribution.loc[append_pointer]=op\n",
    "    except:\n",
    "        missing_idx.append(idx)    \n",
    "# plsa_distribution.loc[0]=[1,2,23,4,5,6,7,8]\n",
    "plsa_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index      13.000000\n",
       "topic_0     0.240907\n",
       "topic_1     0.069340\n",
       "topic_2     0.048070\n",
       "topic_3     0.069163\n",
       "topic_4     0.505741\n",
       "topic_5     0.033697\n",
       "topic_6     0.033083\n",
       "Name: 13, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plsa_distribution.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa_distribution.to_csv(\"plsa_outputs/news_20/7_topics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
