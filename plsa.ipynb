{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Define the number of iterations\n",
    "num_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document-term matrix\n",
    "doc_term_matrix = np.array([[1, 0, 2, 3, 0],\n",
    "                            [0, 1, 1, 0, 1],\n",
    "                            [2, 0, 3, 1, 0],\n",
    "                            [0, 1, 2, 0, 1],\n",
    "                            [1, 1, 0, 1, 1]])\n",
    "\n",
    "# Normalize the document-term matrix\n",
    "doc_term_matrix_norm = doc_term_matrix / np.sum(doc_term_matrix, axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the topic-word probability matrix\n",
    "topic_word_prob = np.random.rand(num_topics, doc_term_matrix.shape[1])\n",
    "topic_word_prob /= np.sum(topic_word_prob, axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document-topic probability matrix\n",
    "doc_topic_prob = np.random.rand(doc_term_matrix.shape[0], num_topics)\n",
    "doc_topic_prob /= np.sum(doc_topic_prob, axis=1, keepdims=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic-Word Probability Matrix:\n",
      " [[4.31198384e-046 4.30226891e-219 1.68669983e-017 1.00000000e+000\n",
      "  2.08213512e-219]\n",
      " [9.73230874e-003 3.66336819e-001 0.00000000e+000 2.31110680e-001\n",
      "  3.92820192e-001]\n",
      " [0.00000000e+000 1.67056688e-001 7.29502906e-001 0.00000000e+000\n",
      "  1.03440405e-001]\n",
      " [3.55457100e-002 3.72169264e-003 0.00000000e+000 7.42347021e-001\n",
      "  2.18385577e-001]\n",
      " [3.31904248e-052 2.35866534e-051 1.00000000e+000 5.98228953e-079\n",
      "  5.07919422e-053]]\n",
      "\n",
      "Document-Topic Probability Matrix:\n",
      " [[1.00000000e+000 2.24810345e-061 5.24024810e-033 1.95096121e-013\n",
      "  2.12614935e-017]\n",
      " [0.00000000e+000 2.12044010e-013 8.09904250e-001 2.37231219e-063\n",
      "  1.90095750e-001]\n",
      " [3.43837754e-045 5.47547610e-107 3.94922776e-016 1.14807150e-057\n",
      "  1.00000000e+000]\n",
      " [0.00000000e+000 4.87292719e-043 1.21289738e-007 1.50011449e-092\n",
      "  9.99999879e-001]\n",
      " [4.87889565e-003 6.42898779e-001 5.84641050e-056 3.52222326e-001\n",
      "  0.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of iterations\n",
    "num_iterations = 100\n",
    "# Iterate between the E-step and M-step\n",
    "for i in range(num_iterations):\n",
    "    # E-step: compute the expected counts\n",
    "    expected_counts = np.zeros((doc_term_matrix.shape[0], num_topics, doc_term_matrix.shape[1]))\n",
    "    for d in range(doc_term_matrix.shape[0]):\n",
    "        for w in range(doc_term_matrix.shape[1]):\n",
    "            for z in range(num_topics):\n",
    "                expected_counts[d, z, w] = doc_term_matrix_norm[d, w] * topic_word_prob[z, w] * doc_topic_prob[d, z]\n",
    "        expected_counts[d] /= np.sum(expected_counts[d])\n",
    "    \n",
    "    # M-step: update the topic-word and document-topic probabilities\n",
    "    for z in range(num_topics):\n",
    "        for w in range(doc_term_matrix.shape[1]):\n",
    "            topic_word_prob[z, w] = np.sum(expected_counts[:, z, w])\n",
    "        topic_word_prob[z] /= np.sum(topic_word_prob[z])\n",
    "    for d in range(doc_term_matrix.shape[0]):\n",
    "        for z in range(num_topics):\n",
    "            doc_topic_prob[d, z] = np.sum(expected_counts[d, z])\n",
    "        doc_topic_prob[d] /= np.sum(doc_topic_prob[d])\n",
    "\n",
    "# Print the topic-word probability matrix\n",
    "print(\"Topic-Word Probability Matrix:\\n\", topic_word_prob)\n",
    "\n",
    "# Print the document-topic probability matrix\n",
    "print(\"\\nDocument-Topic Probability Matrix:\\n\", doc_topic_prob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
