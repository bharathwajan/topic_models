{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Define the number of iterations\n",
    "num_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.        , 0.33333333, 0.5       , 0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.        , 0.33333333],\n",
       "       [0.33333333, 0.        , 0.5       , 0.16666667, 0.        ],\n",
       "       [0.        , 0.25      , 0.5       , 0.        , 0.25      ],\n",
       "       [0.25      , 0.25      , 0.        , 0.25      , 0.25      ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the document-term matrix\n",
    "doc_term_matrix = np.array([[1, 0, 2, 3, 0],\n",
    "                            [0, 1, 1, 0, 1],\n",
    "                            [2, 0, 3, 1, 0],\n",
    "                            [0, 1, 2, 0, 1],\n",
    "                            [1, 1, 0, 1, 1]])\n",
    "\n",
    "# Normalize the document-term matrix\n",
    "doc_term_matrix_norm = doc_term_matrix / np.sum(doc_term_matrix, axis=1, keepdims=True)\n",
    "doc_term_matrix_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20423176, 0.20474642, 0.29897323, 0.04897592, 0.24307267],\n",
       "       [0.24045383, 0.2614242 , 0.07662477, 0.10963181, 0.31186539],\n",
       "       [0.31454333, 0.21745909, 0.14992339, 0.12287203, 0.19520216],\n",
       "       [0.25884301, 0.26308928, 0.18526906, 0.08505402, 0.20774463],\n",
       "       [0.12351062, 0.10727952, 0.39037787, 0.29632321, 0.08250878]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the topic-word probability matrix\n",
    "topic_word_prob = np.random.rand(num_topics, doc_term_matrix.shape[1])\n",
    "topic_word_prob /= np.sum(topic_word_prob, axis=1, keepdims=True)\n",
    "topic_word_prob #probability of word taking each topics w1 can have [0.20423176, 0.20474642, 0.29897323, 0.04897592, 0.24307267] for 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13184428, 0.30317039, 0.1678003 , 0.13487425, 0.26231078],\n",
       "       [0.06722848, 0.16997049, 0.34459889, 0.08298197, 0.33522017],\n",
       "       [0.35416569, 0.30401898, 0.20655707, 0.01482401, 0.12043425],\n",
       "       [0.13519912, 0.06252038, 0.17082766, 0.36745826, 0.26399458],\n",
       "       [0.30359387, 0.29423659, 0.03409148, 0.13421988, 0.23385817]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the document-topic probability matrix\n",
    "doc_topic_prob = np.random.rand(doc_term_matrix.shape[0], num_topics)\n",
    "doc_topic_prob /= np.sum(doc_topic_prob, axis=1, keepdims=True)\n",
    "\n",
    "doc_topic_prob #probability of document taking each topics ,doc1 can have [0.20423176, 0.20474642, 0.29897323, 0.04897592, 0.24307267] for 5 topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic-Word Probability Matrix:\n",
      " [[4.31198384e-046 4.30226891e-219 1.68669983e-017 1.00000000e+000\n",
      "  2.08213512e-219]\n",
      " [9.73230874e-003 3.66336819e-001 0.00000000e+000 2.31110680e-001\n",
      "  3.92820192e-001]\n",
      " [0.00000000e+000 1.67056688e-001 7.29502906e-001 0.00000000e+000\n",
      "  1.03440405e-001]\n",
      " [3.55457100e-002 3.72169264e-003 0.00000000e+000 7.42347021e-001\n",
      "  2.18385577e-001]\n",
      " [3.31904248e-052 2.35866534e-051 1.00000000e+000 5.98228953e-079\n",
      "  5.07919422e-053]]\n",
      "\n",
      "Document-Topic Probability Matrix:\n",
      " [[1.00000000e+000 2.24810345e-061 5.24024810e-033 1.95096121e-013\n",
      "  2.12614935e-017]\n",
      " [0.00000000e+000 2.12044010e-013 8.09904250e-001 2.37231219e-063\n",
      "  1.90095750e-001]\n",
      " [3.43837754e-045 5.47547610e-107 3.94922776e-016 1.14807150e-057\n",
      "  1.00000000e+000]\n",
      " [0.00000000e+000 4.87292719e-043 1.21289738e-007 1.50011449e-092\n",
      "  9.99999879e-001]\n",
      " [4.87889565e-003 6.42898779e-001 5.84641050e-056 3.52222326e-001\n",
      "  0.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of iterations\n",
    "num_iterations = 100\n",
    "# Iterate between the E-step and M-step\n",
    "for i in range(num_iterations):\n",
    "    # E-step: compute the expected counts\n",
    "    expected_counts = np.zeros((doc_term_matrix.shape[0], num_topics, doc_term_matrix.shape[1]))\n",
    "    for d in range(doc_term_matrix.shape[0]):\n",
    "        for w in range(doc_term_matrix.shape[1]):\n",
    "            for z in range(num_topics):\n",
    "                expected_counts[d, z, w] = doc_term_matrix_norm[d, w] * topic_word_prob[z, w] * doc_topic_prob[d, z]\n",
    "        expected_counts[d] /= np.sum(expected_counts[d]) #normalization step\n",
    "    \n",
    "    # M-step: update the topic-word and document-topic probabilities\n",
    "    for z in range(num_topics):\n",
    "        for w in range(doc_term_matrix.shape[1]):\n",
    "            topic_word_prob[z, w] = np.sum(expected_counts[:, z, w])\n",
    "        topic_word_prob[z] /= np.sum(topic_word_prob[z])\n",
    "    for d in range(doc_term_matrix.shape[0]):\n",
    "        for z in range(num_topics):\n",
    "            doc_topic_prob[d, z] = np.sum(expected_counts[d, z])\n",
    "        doc_topic_prob[d] /= np.sum(doc_topic_prob[d])\n",
    "\n",
    "# Print the topic-word probability matrix\n",
    "print(\"Topic-Word Probability Matrix:\\n\", topic_word_prob)\n",
    "\n",
    "# Print the document-topic probability matrix\n",
    "print(\"\\nDocument-Topic Probability Matrix:\\n\", doc_topic_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
